{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "\n",
    "# First you must use https://github.com/Tyrrrz/DiscordChatExporter to export the chat logs\n",
    "\n",
    "# dotnet DiscordChatExporter.Cli.dll exportguild -g <server-id> -t \"bot-token\" -f PlainText --parallel 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved links to the extracted_links_may2 directory.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set the input folder and output folder\n",
    "input_folder = \"extracted_links_may\"\n",
    "output_folder = \"extracted_links_may2\"\n",
    "\n",
    "# Initialize a dictionary to store extracted links grouped by domain\n",
    "links_by_domain = defaultdict(list)\n",
    "\n",
    "# Walk through the directory tree\n",
    "for root, dirs, files in os.walk(input_folder):\n",
    "    for filename in files:\n",
    "        # Check if the file has a .txt extension\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Read the input file\n",
    "            with open(file_path, \"r\") as file:\n",
    "                file_contents = file.read()\n",
    "\n",
    "            # Extract links using regular expressions\n",
    "            url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "            links = re.findall(url_pattern, file_contents)\n",
    "            \n",
    "            # Add extracted links to the dictionary\n",
    "            for link in links:\n",
    "                domain = urlparse(link).netloc\n",
    "                if domain not in links_by_domain:\n",
    "                    links_by_domain[domain] = set()\n",
    "                links_by_domain[domain].add(link)\n",
    "\n",
    "# Function to group domains\n",
    "def group_domains(domain):\n",
    "    if domain in ['twitter.com', 't.co']:\n",
    "        return 'twitter'\n",
    "    return domain\n",
    "\n",
    "# Group specific domains and create a 'Misc.txt' file for domains with less than 5 results\n",
    "grouped_links = defaultdict(list)\n",
    "misc_links = []\n",
    "\n",
    "for domain, links in links_by_domain.items():\n",
    "    group = group_domains(domain)\n",
    "    \n",
    "    if len(links) < 5:\n",
    "        misc_links.extend(links)\n",
    "    else:\n",
    "        grouped_links[group].extend(links)\n",
    "\n",
    "if misc_links:\n",
    "    grouped_links[\"Misc\"].extend(misc_links)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the grouped links to separate .txt files\n",
    "for group, links in grouped_links.items():\n",
    "    output_file = os.path.join(output_folder, f\"{group}.txt\")\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        for link in links:\n",
    "            file.write(f\"{link}\\n\")\n",
    "\n",
    "print(f\"Extracted and saved links to the {output_folder} directory.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "def get_toml_path(toml_base_name, base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == toml_base_name:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlsplit(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def trim_url(url):\n",
    "    \"\"\"Trim a GitHub URL to the base organization or repository URL.\"\"\"\n",
    "    url = url.strip()\n",
    "    url = re.sub(r\"\\?.*$\", \"\", url)  # Remove query parameters\n",
    "    url = re.sub(r\"#.*$\", \"\", url)  # Remove fragments\n",
    "    url = url.rstrip(\"/\")\n",
    "    if \"/pull/\" in url:\n",
    "        url = url[: url.index(\"/pull/\")]\n",
    "    if \"/wiki\" in url:\n",
    "        url = url[: url.index(\"/wiki\")]\n",
    "    match = re.search(r\"(https?://github\\.com/[^/]+/[^/]+)\", url)\n",
    "    url = re.sub(r\"[\\)\\.\\*\\\"]+$\", \"\", url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return re.sub(r\"[\\)\\.\\\"]+$\", \"\", url)\n",
    "    \n",
    "def remove_special_chars(url):\n",
    "    return re.sub(r'[\\*\\)]+$', '', url)\n",
    "\n",
    "\n",
    "    \n",
    "output_toml = \"../../../crypto-ecosystems/data/ecosystems/e/ergo-developer-tooling.toml\"\n",
    "ergo_toml = \"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\"\n",
    "# Load the ergo.toml file\n",
    "with open(ergo_toml, \"r\") as file:\n",
    "    ergo_data = toml.load(file)\n",
    "\n",
    "# Load the github.com.txt file\n",
    "with open(\"extracted_links_may2/github.com.txt\", \"r\") as file:\n",
    "    github_links = file.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all subecosystem .toml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 sub ecosystem .toml files found:\n",
      "0 existing organizations and repositories found:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the sub_ecosystems list\n",
    "sub_ecosystems = ergo_data[\"sub_ecosystems\"]\n",
    "\n",
    "# Define the base directory for searching .toml files\n",
    "base_dir = \"/Users/m/Documents/GitHub/crypto-ecosystems/data/ecosystems/\"\n",
    "\n",
    "# Collect .toml files for each sub_ecosystem\n",
    "sub_ecosystem_toml_files = []\n",
    "\n",
    "# Iterate over the sub_ecosystems list\n",
    "for sub_ecosystem in sub_ecosystems:\n",
    "    toml_base_name = sub_ecosystem.lower().replace(\" \", \"-\") + \".toml\"\n",
    "    toml_path = get_toml_path(toml_base_name, base_dir)\n",
    "    if toml_path:\n",
    "        sub_ecosystem_toml_files.append(toml_path)\n",
    "    else:\n",
    "        print(f\"Could not find {toml_base_name}\")\n",
    "\n",
    "\n",
    "# A set to keep track of existing organizations and repositories\n",
    "existing_orgs_and_repos = set()\n",
    "\n",
    "# A set to keep track of existing URLs\n",
    "existing_urls_set = set()\n",
    "\n",
    "# Print the .toml files corresponding to the sub_ecosystems\n",
    "print(len(sub_ecosystem_toml_files), 'sub ecosystem .toml files found:')\n",
    "\n",
    "\n",
    "print(len(existing_orgs_and_repos), 'existing organizations and repositories found:')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the sub ecosystem .toml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624 existing URLs found\n",
      "624 existing organizations and repositories found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each sub ecosystem .toml file\n",
    "for toml_file_pattern in sub_ecosystem_toml_files:\n",
    "    for toml_file in glob.glob(toml_file_pattern):\n",
    "        # Load the .toml file\n",
    "        with open(toml_file, \"r\") as file:\n",
    "            data = toml.load(file)\n",
    "\n",
    "        # Extract existing URLs from the .toml file\n",
    "        existing_urls = []\n",
    "        if \"github_organizations\" in data:\n",
    "            existing_urls += data[\"github_organizations\"]\n",
    "        if \"repo\" in data:\n",
    "            existing_urls += [repo[\"url\"] for repo in data[\"repo\"]]\n",
    "\n",
    "        # Extract organization and repository names from existing URLs\n",
    "        for url in existing_urls:\n",
    "            #print(url)\n",
    "            trimmed_url = trim_url(url)\n",
    "            \n",
    "            if is_valid_url(trimmed_url):\n",
    "                # Extract the organization and repository names from the URL\n",
    "                match = re.search(r\"github.com/([^/]+)(?:/([^/]+))?\", trimmed_url)\n",
    "\n",
    "                # If the URL is a valid GitHub URL, add it to the set\n",
    "                if match:\n",
    "                    org_and_repo = match.groups()\n",
    "                    existing_orgs_and_repos.add(org_and_repo)\n",
    "                    existing_urls_set.add(trimmed_url)\n",
    "                \n",
    "\n",
    "                # Add the organization to the set as well, if it is in github_organizations\n",
    "                for org in data.get(\"github_organizations\", []):\n",
    "                    if trimmed_url.startswith(org):\n",
    "                        org_name = urlsplit(org).path.strip('/')\n",
    "                        existing_orgs_and_repos.add((org_name, None))\n",
    "                        #print(org_name, 'Added!')\n",
    "\n",
    "print(len(existing_urls_set), \"existing URLs found\")\n",
    "print(len(existing_orgs_and_repos), \"existing organizations and repositories found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find missing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify organizations to ignore\n",
    "ignore_orgs = {\n",
    "            \"fusesource\", 'cardano-community', 'DefiLlama', 'arc53', 'randlabs', 'algorandfoundation', 'w3f',\n",
    "            'halsafar', 'Arman92', 'electric-capital', 'IndeedMiners', 'download13', \n",
    "            'firstcontributions', 'Xilinx', 'ethereum-optimism', 'ExpediaGroup',\n",
    "            'trustwallet', 'BLAKE3-team', 'ZSLP', 'bitcoin', 'maticnetwork', 'features', \n",
    "            'etclabscore', 'coinfoundry', 'Lolliedieb', 'lustefaniak', 'outline', 'dotnet', 'discord', \n",
    "            'Uniswap', 'trexminer', 'minershive', 'NebuTech', 'doktor83', 'AUTOMATIC1111',\n",
    "            'arnabk', 'YouMinerDev', 'freebsd', 'scalameta', 'GetScatter', 'square', 'outcaste-io', 'marketplace', \n",
    "            'todxx', 'oliverw', 'jpg-store', 'paritytech', 'dogecoin', 'tpruvot', 'nicehash', 'zack-bitcoin', \n",
    "            '42wim', 'RainbowMiner', 'rainbowminer', 'hexresearch', 'coursier', 'minswap', 'trending', 'GTON-capital', \n",
    "            'bruno-garcia', 'ZeroSync', 'advisories', 'RavenCommunity', 'CREDITSCOM', 'jlopp', 'penk', 'cexplorer', \n",
    "            'trezor', 'SChernykh', 'ElementsProject', 'honungsburk', 'monerobook', 'kaspanet', 'mit-dci', \n",
    "            'chadouming', 'tvanepps', 'i1skn', 'nanopool', 'matrix-org', 'Chia-Network', 'hanstjua', 'nvm-sh', \n",
    "            'brave', '045bkp', 'trezor', 'twitter', 'WinterTFG0', 'input-output-hk', 'chakra-ui', 'bcgame-project', \n",
    "            'ethereum', 'TremendouslyHighFrequency', 'menonsamir', 'althea-net', 'scritcash', 'btcsuite', \n",
    "            'certusone', 'coreybutler', 'certusone', 'bzminer', 'VeriConomy', 'braposo', 'ccgarant', 'TrustyJAID', \n",
    "            'ScorexFoundation',  'rust-bitcoin', 'non', 'WyvernTKC', 'C4K3', 'rust-lang', 'coinexcom', 'coreyphillips',\n",
    "            'JohnLaw2', 'nervosnetwork', 's-nomp', 'sp-hash', 'zone117x', 'rafaelsorto', 'cointastical',\n",
    "            'ShiftLeftSecurity', 'MrMaxweII', 'forknote', 'AtomicLoans', 'randao', 'aljazceru', 'carbon-language',\n",
    "            'Dav-Git', 'starkware-libs', 'reach-sh', 'reflexer-labs', 'LemmyNet', 'swagger-api', 'BPSAA', 'Androz2091', \n",
    "            'aeternity', 'alt3', 'NixOS', 'OpenAPITools', 'Mikerah', 'MohdGhazanfar', 'aionnetwork', 'amanbthakkar', \n",
    "            'BySergeyDev', 'sangria-graphql', 'soullesscomputerboy', 'sahharYoucef', 'oxen-io', 'LedgerHQ', 'Giveth', 'SecuraBV', \n",
    "            'TwiN', 'SundaeSwap-finance', 'minernl', 'WinterTFG', 'GENERALBYTESCOM', 'BlockchainCommons', 'JimmyHoffa', \n",
    "            'JulianKemmerer', 'kyuupichan', 'sdaveas', 'lightbend', 'toncoinpool', 'QuarkChain', 'agjell', 'pluja', 'xwiki-labs', 'MystenLabs', 'strapi',\n",
    "            'AlphaX-Projects', 'ma-ha', 'rsmmnt', 'arduino', 'electron', 'rsksmart', 'nymtech', 'martin-mx', 'TurtleNetwork', \n",
    "            'C4K3', 'BLAKE2', 'minio', 'ethereum-mining', 'kadena-io', 'sponsors', 'hwchase17', 'develsoftware', \n",
    "            'wavesplatform', 'plebbit', 'dcSpark', 'john-light', 'JetBrains', 'epidemics-scepticism', 'xtekky', \n",
    "            'Eliovp', 'libp2p', 'simerplaha', 'ossu', 'dashevo', 'ethereumclassic', 'ipfs',\n",
    "            'btclinux', 'orgs', 'ghostdogpr', 'yuriy0803', 'portable-scala', 'KomodoPlatform',\n",
    "            'tiangolo', 'bwbush', 'CLRX', 'OhGodPet', 'YfryTchsGD', 'Comcast', 'atomiclabs', 'pikvm', 'Polkadex-Substrate', \n",
    "            'PyO3', 'Astodialo', 'SpaceXpanse', 'sbt', 'alephium', 'emeraldpay', 'spantaleev', 'japgolly',\n",
    "            'FgForrest', 'jrbender', 'berry-pool', 'Gravity-Tech', 'obolflip', 'SuSy-One', 'OhGodAPet', 'Balbin-Labs',\n",
    "            '.insteadOf', 'hyperledger-labs', 'spaceswapio', 'SpaceSwapio', 'search', 'prometheus', 'DevSCNinja', 'aragogi', 'bitcoincashorg', 'bcgit', 'gemlink',\n",
    "            'robkorn', 'sininen-taivas', 'ergo', 'zawy12', 'rooooooooob', 'akyo8', 'arcnet', 'adventurersdao',\n",
    "            'gsblabsio', 'ergop', 'Emurgo', 'trailofbits', 'massgravel', 'cardano-foundation', 'getumbrel', 'dessant', \n",
    "            'ergoplatform',  'ergomixer', 'ergoMixer', 'teddy-swap', 'apps', 'context-labs', 'deezer', 'etho-black',\n",
    "            'bradtraversy', 'CardanoSolutions', 'swarmpit' , 'thedevs-network', 'frappe', 'TheDuckCow', 'bdkent', 'bitcoinj', 'xwiki-contrib', \n",
    "            'FiloSottile', 'ergoplatf', 'inkytonik', 'beeper', 'vuejs', 'Rafael-SV', 'bcndev', 'damus-io', 'Cog-Creators', 'github', 'ZcashFoundation', 'EdenBlockVC',\n",
    "            \n",
    "            }\n",
    "\n",
    "# Convert all organization names to lowercase in the ignore_orgs set\n",
    "ignore_orgs = {org.lower() for org in ignore_orgs}\n",
    "\n",
    "ignore_repos = {\n",
    "    \"Atomicity\",\n",
    "    \"Au\",\n",
    "    \"sigma-usd\", \n",
    "    \"dockerhub-webhooks\",\n",
    "    \"dedao\",\n",
    "    \"forecasting\",\n",
    "}\n",
    "\n",
    "# Convert all repository names to lowercase in the ignore_repos set\n",
    "ignore_repos = {repo.lower() for repo in ignore_repos}\n",
    "\n",
    "# Convert all existing organization and repository names to lowercase\n",
    "existing_orgs_and_repos = {(org.lower(), repo.lower() if repo else None) for org, repo in existing_orgs_and_repos}\n",
    "\n",
    "# Create a set of existing URLs in lowercase\n",
    "existing_urls_set_lower = {url.lower() for url in existing_urls_set}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to missing_links https://github.com/mhssamadani/ErgoStratumServer.\n",
      "Adding to missing_links https://github.com/Luivatra/oracle-core.git,\n",
      "Adding to missing_links https://github.com/iandebeer/ergo-castanet.\n",
      "Adding to missing_links https://github.com/anon-br/ledger-ergo-js.\n",
      "Adding to missing_links https://github.com/mhssamadani/ErgoStratumServer>\n",
      "Adding to missing_links https://github.com/https://ergoplatform.org/en/blog/2022-02-09-ergos-hybrid-method-for-counting-costs/platform/grow-ergo/blob/maihttps://ergoplatform.org/en/blog/2022-02-18-ergo-explainer-storage-rent/www.reddit.chttps://docs.ergoplatform.com/onauts/comments/suxdfhttps://www.reddit.com/r/https://www.reddit.com/r/ergonauts/comments/spclyi/nautilus/s/comments/stvtq3/ergo_summit_feb_1723/_has_a_bright_future/ptocream\n",
      "Adding to missing_links https://github.com/Eeysirhc\n",
      "Adding to missing_links https://github.com/ErgoWallet/ergowallet-desktop,\n",
      "Adding to missing_links https://github.com/ccxt/ccxt,\n",
      "Adding to missing_links https://github.com/ThierryM1212/blobs-topia_\n",
      "Adding to missing_links https://github.com/Luivatra/ergo.\n",
      "Adding to missing_links https://github.com/kushti/dexy-stable,\n",
      "Adding to missing_links https://github.com/ross-weir/ogre>\n",
      "Adding to missing_links https://github.com/abchrisxyz/ergo-setup>\n",
      "Adding to missing_links https://github.com/txpipe>\n",
      "Adding to missing_links https://github.com/ross-weir/ergo-ledger-dev,\n",
      "Adding to missing_links https://github.com/anon-real/ErgoUtils.\n",
      "Adding to missing_links https://github.com/aslesarenko/ergo-tool,\n",
      "Adding to missing_links https://github.com/mowreez/ergo-asset-...\n",
      "Adding to missing_links https://github.com/ThierryM1212/SAFEW>\n",
      "Adding to missing_links https://github.com/abchrisxyz/ergowatch>\n",
      "Adding to missing_links https://github.com/evaqing/content-locales-cn.json-\n",
      "Adding to missing_links https://github.com/ThierryM1212/transaction-builder.\n",
      "Adding to missing_links https://github.com/ThierryM1212/ergo-token-min...\n",
      "Adding to missing_links https://github.com/andrehafner/sigmavote>\n",
      "Adding to missing_links https://github.com/zkastn/ergo-raffle-bot,\n",
      "Adding to missing_links https://github.com/mhssamadani/Autolykos2_NV_Miner.\n",
      "Adding to missing_links https://github.com/your_name/sigma-usd)),\n",
      "Adding to missing_links https://github.com/andrehafner/my.ergoport.dev>\n",
      "Adding to missing_links https://github.com/aslesarenko/ergo-android.\n",
      "Adding to missing_links https://github.com/anon-real/ErgoTeam.\n",
      "Adding to missing_links https://github.com/nirvanush/isomorphic-wallet,\n",
      "Adding to missing_links https://github.com/settings/tokens>\n",
      "Adding to missing_links https://github.com/nirvanush/whale-alerts-twit...\n",
      "Adding to missing_links https://github.com/kushti/flp432.\n",
      "Adding to missing_links https://github.com/tesseract-one/ledger-app-er...\n",
      "Adding to missing_links https://github.com/mgpai22/ergo-explorer-testnet,\n",
      "Adding to missing_links https://github.com/crystoll/export-erg-transactions...\n",
      "Adding to missing_links https://github.com/mgpai2\n",
      "Adding to missing_links https://github.com/abchrisxyz/ergowatch-ui>\n",
      "659 existing organizations and repositories found\n",
      "40 missing links found\n"
     ]
    }
   ],
   "source": [
    "# Compare the github.com.txt file and create a list of missing links\n",
    "missing_links = set()\n",
    "\n",
    "for link in github_links:\n",
    "    link = remove_special_chars(link.strip())\n",
    "    trimmed_link = trim_url(link)\n",
    "    if is_valid_url(trimmed_link):\n",
    "        match = re.search(r\"github.com/([^/]+)(?:/([^/]+))?\", trimmed_link)\n",
    "\n",
    "        # If the URL is a valid GitHub URL, check if it is already in the list of existing organizations and repositories\n",
    "        if match:\n",
    "            # Extract the organization and repository names from the URL\n",
    "            org_and_repo = match.groups()\n",
    "            # Convert the organization and repository names to lowercase\n",
    "            org_and_repo = (org_and_repo[0].lower(), org_and_repo[1].lower() if org_and_repo[1] else None)\n",
    "\n",
    "            # Check if the organization is in the ignore list\n",
    "            if org_and_repo[0] not in ignore_orgs:\n",
    "\n",
    "                # Check if the repository is in the ignore list\n",
    "                if org_and_repo[1] is None or org_and_repo[1] not in ignore_repos:\n",
    "\n",
    "                    # Check if the organization exists in the existing_orgs_and_repos set\n",
    "                    org_exists = any(existing_org == org_and_repo[0] and existing_repo is None for existing_org, existing_repo in existing_orgs_and_repos)\n",
    "\n",
    "                    # If the organization doesn't exist, and the lowercase URL is not in the existing_urls_set_lower, add it to the missing_links\n",
    "                    if not org_exists and trimmed_link.lower() not in existing_urls_set_lower:\n",
    "                        print(\"Adding to missing_links\", trimmed_link)\n",
    "                        missing_links.add(trimmed_link)\n",
    "                        existing_orgs_and_repos.add(org_and_repo)\n",
    "                        #print(f\"Added {org_and_repo} to existing_orgs_and_repos\")\n",
    "\n",
    "# Convert the missing_links set back to a list\n",
    "missing_links = list(missing_links)\n",
    "\n",
    "print(len(existing_orgs_and_repos), \"existing organizations and repositories found\")\n",
    "print(len(missing_links), \"missing links found\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ergo.toml file\n",
    "with open(output_toml, \"r\") as file:\n",
    "    ergo_data = toml.load(file)\n",
    "\n",
    "# Add the missing links to ergo.toml\n",
    "for link in missing_links:\n",
    "    if is_valid_url(link):\n",
    "        ergo_data[\"repo\"].append({\"url\": link})\n",
    "\n",
    "# Save the changes in the existing ergo.toml file\n",
    "with open(output_toml, \"w\") as file:\n",
    "    toml.dump(ergo_data, file)\n",
    "\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post sanitisation since I can't be bothered to fix the input.\n",
    "\n",
    "with open(output_toml, \"r\") as file:\n",
    "    data = toml.load(file)\n",
    "\n",
    "data[\"repo\"] = [d for d in data[\"repo\"] if re.match(r\"^https?://.+[a-zA-Z0-9]$\", d[\"url\"])]\n",
    "\n",
    "with open(output_toml, \"w\") as file:\n",
    "    toml.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyGithub\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "\n",
    "# Set your GitHub Personal Access Token\n",
    "GITHUB_TOKEN = \"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\"\n",
    "gh = Github(GITHUB_TOKEN)\n",
    "\n",
    "def get_repo_details(repo):\n",
    "    try:\n",
    "        contributors = repo.get_contributors().totalCount\n",
    "    except Exception:\n",
    "        contributors = 0\n",
    "\n",
    "    return {\n",
    "        \"full_name\": repo.full_name,\n",
    "        \"published_date\": repo.created_at,\n",
    "        \"last_updated\": repo.updated_at,\n",
    "        \"forks\": repo.forks_count,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"contributors\": contributors,\n",
    "    }\n",
    "\n",
    "def export_to_csv(data, filename):\n",
    "    header = data[0].keys()\n",
    "    with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "repositories = []\n",
    "organizations = []\n",
    "\n",
    "for org_name, repo_name in existing_orgs_and_repos:\n",
    "    if repo_name is None:\n",
    "        try:\n",
    "            org = gh.get_organization(org_name)\n",
    "            org_repos = org.get_repos()\n",
    "            organizations.append({\n",
    "                \"organization\": org_name,\n",
    "                \"repo_count\": org_repos.totalCount,\n",
    "            })\n",
    "\n",
    "            for repo in org_repos:\n",
    "                repositories.append(get_repo_details(repo))\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching organization '{org_name}': {str(e)}\")\n",
    "    else:\n",
    "        try:\n",
    "            repo = gh.get_repo(f\"{org_name}/{repo_name}\")\n",
    "            repositories.append(get_repo_details(repo))\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching repository '{org_name}/{repo_name}': {str(e)}\")\n",
    "\n",
    "print(f\"Total organizations: {len(organizations)}\")\n",
    "print(f\"Total repositories: {len(repositories)}\")\n",
    "\n",
    "# Export the data to CSV files\n",
    "export_to_csv(organizations, \"organizations.csv\")\n",
    "export_to_csv(repositories, \"repositories.csv\")\n",
    "\n",
    "print(\"Data has been exported to organizations.csv and repositories.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import glob\n",
    "import toml\n",
    "\n",
    "sub_ecosystem_toml_files = glob.glob(\"../../../crypto-ecosystems/data/ecosystems/*/ecosystem.toml\")\n",
    "\n",
    "existing_orgs_and_repos = set()\n",
    "\n",
    "for toml_file in sub_ecosystem_toml_files:\n",
    "    with open(toml_file, \"r\") as file:\n",
    "        data = toml.load(file)\n",
    "\n",
    "        if \"github_organizations\" in data:\n",
    "            for org in data[\"github_organizations\"]:\n",
    "                existing_orgs_and_repos.add((org.strip(\"/\").split(\"/\")[-1], None))\n",
    "\n",
    "        if \"repo\" in data:\n",
    "            for repo in data[\"repo\"]:\n",
    "                url = repo.get(\"url\")\n",
    "                if url:\n",
    "                    trimmed_url = url.strip(\"/\").replace(\"https://github.com/\", \"\")\n",
    "                    org_and_repo = tuple(trimmed_url.split(\"/\")[:2])\n",
    "                    existing_orgs_and_repos.add(org_and_repo)\n",
    "\n",
    "with open(\"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\", \"r\") as file:\n",
    "    ergo_data = toml.load(file)\n",
    "\n",
    "new_repos = []\n",
    "for repo in ergo_data.get(\"repo\", []):\n",
    "    url = repo.get(\"url\")\n",
    "    if url:\n",
    "        trimmed_url = url.strip(\"/\").replace(\"https://github.com/\", \"\")\n",
    "        org_and_repo = tuple(trimmed_url.split(\"/\")[:2])\n",
    "        if org_and_repo not in existing_orgs_and_repos:\n",
    "            new_repos.append(repo)\n",
    "\n",
    "ergo_data[\"repo\"] = new_repos\n",
    "\n",
    "with open(\"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\", \"w\") as file:\n",
    "    toml.dump(ergo_data, file)\n",
    "\n",
    "def get_toml_path(toml_base_name, base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == toml_base_name:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import toml\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# Load ergo.toml\n",
    "with open(\"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\", \"r\") as f:\n",
    "    ergo_data = toml.load(f)\n",
    "\n",
    "# Get sub-ecosystems\n",
    "sub_ecosystems = ergo_data[\"sub_ecosystems\"]\n",
    "\n",
    "# Get repo URLs\n",
    "existing_urls_set = set()\n",
    "for repo in ergo_data.get(\"repo\", []):\n",
    "    existing_urls_set.add(repo.get(\"url\"))\n",
    "\n",
    "# Define the base directory for searching .toml files\n",
    "base_dir = \"../../../crypto-ecosystems/data/ecosystems/\"\n",
    "\n",
    "# Check each sub-ecosystem for duplicate URLs\n",
    "for sub_ecosystem in sub_ecosystems:\n",
    "    toml_base_name = sub_ecosystem.lower().replace(\" \", \"_\") + \".toml\"\n",
    "    toml_path = get_toml_path(toml_base_name, base_dir)\n",
    "    print(sub_ecosystem)\n",
    "    print(toml_path)\n",
    "    if toml_path:\n",
    "        with open(toml_path, \"r\") as f:\n",
    "            data = toml.load(f)\n",
    "        if \"repo\" in data:\n",
    "            for repo in data[\"repo\"]:\n",
    "                if repo.get(\"url\") in existing_urls_set:\n",
    "                    # Remove the duplicate URL from the repo section of ergo.toml\n",
    "                    ergo_data[\"repo\"] = [r for r in ergo_data.get(\"repo\", []) if r.get(\"url\") != repo.get(\"url\")]\n",
    "\n",
    "\n",
    "# Save the changes to ergo.toml\n",
    "with open(\"output.toml\", \"w\") as f:\n",
    "    toml.dump(ergo_data, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
