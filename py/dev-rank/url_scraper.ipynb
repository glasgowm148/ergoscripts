{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "\n",
    "# First you must use https://github.com/Tyrrrz/DiscordChatExporter to export the chat logs\n",
    "\n",
    "# dotnet DiscordChatExporter.Cli.dll exportguild -g <server-id> -t \"bot-token\" -f PlainText --parallel 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved links to the extracted_links directory.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set the input folder and output folder\n",
    "input_folder = \"chats\"\n",
    "output_folder = \"extracted_links\"\n",
    "\n",
    "# Initialize a dictionary to store extracted links grouped by domain\n",
    "links_by_domain = defaultdict(list)\n",
    "\n",
    "# Walk through the directory tree\n",
    "for root, dirs, files in os.walk(input_folder):\n",
    "    for filename in files:\n",
    "        # Check if the file has a .txt extension\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Read the input file\n",
    "            with open(file_path, \"r\") as file:\n",
    "                file_contents = file.read()\n",
    "\n",
    "            # Extract links using regular expressions\n",
    "            url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "            links = re.findall(url_pattern, file_contents)\n",
    "            \n",
    "            # Add extracted links to the dictionary\n",
    "            for link in links:\n",
    "                domain = urlparse(link).netloc\n",
    "                if domain not in links_by_domain:\n",
    "                    links_by_domain[domain] = set()\n",
    "                links_by_domain[domain].add(link)\n",
    "\n",
    "# Function to group domains\n",
    "def group_domains(domain):\n",
    "    if domain in ['twitter.com', 't.co']:\n",
    "        return 'twitter'\n",
    "    return domain\n",
    "\n",
    "# Group specific domains and create a 'Misc.txt' file for domains with less than 5 results\n",
    "grouped_links = defaultdict(list)\n",
    "misc_links = []\n",
    "\n",
    "for domain, links in links_by_domain.items():\n",
    "    group = group_domains(domain)\n",
    "    \n",
    "    if len(links) < 5:\n",
    "        misc_links.extend(links)\n",
    "    else:\n",
    "        grouped_links[group].extend(links)\n",
    "\n",
    "if misc_links:\n",
    "    grouped_links[\"Misc\"].extend(misc_links)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the grouped links to separate .txt files\n",
    "for group, links in grouped_links.items():\n",
    "    output_file = os.path.join(output_folder, f\"{group}.txt\")\n",
    "    \n",
    "    with open(output_file, \"w\") as file:\n",
    "        for link in links:\n",
    "            file.write(f\"{link}\\n\")\n",
    "\n",
    "print(f\"Extracted and saved links to the {output_folder} directory.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "def get_toml_path(toml_base_name, base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == toml_base_name:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlsplit(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def trim_url(url):\n",
    "    \"\"\"Trim a GitHub URL to the base organization or repository URL.\"\"\"\n",
    "    url = url.strip()\n",
    "    url = re.sub(r\"\\?.*$\", \"\", url)  # Remove query parameters\n",
    "    url = re.sub(r\"#.*$\", \"\", url)  # Remove fragments\n",
    "    url = url.rstrip(\"/\")\n",
    "    if \"/pull/\" in url:\n",
    "        url = url[: url.index(\"/pull/\")]\n",
    "    if \"/wiki\" in url:\n",
    "        url = url[: url.index(\"/wiki\")]\n",
    "    match = re.search(r\"(https?://github\\.com/[^/]+/[^/]+)\", url)\n",
    "    url = re.sub(r\"[\\)\\.\\*\\\"]+$\", \"\", url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return re.sub(r\"[\\)\\.\\\"]+$\", \"\", url)\n",
    "    \n",
    "def remove_special_chars(url):\n",
    "    return re.sub(r'[\\*\\)]+$', '', url)\n",
    "\n",
    "\n",
    "    \n",
    "output_toml = \"../../../crypto-ecosystems/data/ecosystems/e/ergo-developer-tooling.toml\"\n",
    "ergo_toml = \"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\"\n",
    "# Load the ergo.toml file\n",
    "with open(ergo_toml, \"r\") as file:\n",
    "    ergo_data = toml.load(file)\n",
    "\n",
    "# Load the github.com.txt file\n",
    "with open(\"extracted_links/github.com.txt\", \"r\") as file:\n",
    "    github_links = file.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all subecosystem .toml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 sub ecosystem .toml files found:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the sub_ecosystems list\n",
    "sub_ecosystems = ergo_data[\"sub_ecosystems\"]\n",
    "\n",
    "# Define the base directory for searching .toml files\n",
    "base_dir = \"/Users/m/Documents/GitHub/crypto-ecosystems/data/ecosystems/\"\n",
    "\n",
    "# Collect .toml files for each sub_ecosystem\n",
    "sub_ecosystem_toml_files = []\n",
    "\n",
    "# Iterate over the sub_ecosystems list\n",
    "for sub_ecosystem in sub_ecosystems:\n",
    "    toml_base_name = sub_ecosystem.lower().replace(\" \", \"-\") + \".toml\"\n",
    "    toml_path = get_toml_path(toml_base_name, base_dir)\n",
    "    if toml_path:\n",
    "        sub_ecosystem_toml_files.append(toml_path)\n",
    "    else:\n",
    "        print(f\"Could not find {toml_base_name}\")\n",
    "\n",
    "# Print the .toml files corresponding to the sub_ecosystems\n",
    "print(len(sub_ecosystem_toml_files), 'sub ecosystem .toml files found:')\n",
    "\n",
    "# A set to keep track of existing organizations and repositories\n",
    "existing_orgs_and_repos = set()\n",
    "\n",
    "# A set to keep track of existing URLs\n",
    "existing_urls_set = set()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the sub ecosystem .toml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584 existing URLs found\n",
      "584 existing organizations and repositories found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process each sub ecosystem .toml file\n",
    "for toml_file_pattern in sub_ecosystem_toml_files:\n",
    "    for toml_file in glob.glob(toml_file_pattern):\n",
    "        # Load the .toml file\n",
    "        with open(toml_file, \"r\") as file:\n",
    "            data = toml.load(file)\n",
    "\n",
    "        # Extract existing URLs from the .toml file\n",
    "        existing_urls = []\n",
    "        if \"github_organizations\" in data:\n",
    "            existing_urls += data[\"github_organizations\"]\n",
    "        if \"repo\" in data:\n",
    "            existing_urls += [repo[\"url\"] for repo in data[\"repo\"]]\n",
    "\n",
    "        # Extract organization and repository names from existing URLs\n",
    "        for url in existing_urls:\n",
    "            #print(url)\n",
    "            trimmed_url = trim_url(url)\n",
    "            \n",
    "            if is_valid_url(trimmed_url):\n",
    "                # Extract the organization and repository names from the URL\n",
    "                match = re.search(r\"github.com/([^/]+)(?:/([^/]+))?\", trimmed_url)\n",
    "\n",
    "                # If the URL is a valid GitHub URL, add it to the set\n",
    "                if match:\n",
    "                    org_and_repo = match.groups()\n",
    "                    existing_orgs_and_repos.add(org_and_repo)\n",
    "                    existing_urls_set.add(trimmed_url)\n",
    "                \n",
    "\n",
    "                # Add the organization to the set as well, if it is in github_organizations\n",
    "                for org in data.get(\"github_organizations\", []):\n",
    "                    if trimmed_url.startswith(org):\n",
    "                        org_name = urlsplit(org).path.strip('/')\n",
    "                        existing_orgs_and_repos.add((org_name, None))\n",
    "                        #print(org_name, 'Added!')\n",
    "\n",
    "print(len(existing_urls_set), \"existing URLs found\")\n",
    "print(len(existing_orgs_and_repos), \"existing organizations and repositories found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find missing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify organizations to ignore\n",
    "ignore_orgs = {\n",
    "            \"fusesource\", 'cardano-community', 'DefiLlama', \n",
    "            'halsafar', 'Arman92', 'electric-capital', 'IndeedMiners', \n",
    "            'firstcontributions', 'Xilinx', 'ethereum-optimism', 'ExpediaGroup',\n",
    "            'trustwallet', 'BLAKE3-team', 'ZSLP', 'bitcoin', 'maticnetwork',\n",
    "            'etclabscore', 'coinfoundry', 'Lolliedieb', 'lustefaniak',\n",
    "            'Uniswap', 'trexminer', 'minershive', 'NebuTech', 'doktor83',\n",
    "            'arnabk', 'YouMinerDev', 'freebsd', 'scalameta', 'GetScatter',\n",
    "            'todxx', 'oliverw', 'jpg-store', 'paritytech', 'dogecoin',\n",
    "            '42wim', 'RainbowMiner', 'rainbowminer', 'doktor83', 'hexresearch',\n",
    "            'bruno-garcia', 'ZeroSync', 'advisories', 'RavenCommunity',\n",
    "            'trezor', 'SChernykh', 'ElementsProject', 'honungsburk',\n",
    "            'chadouming', 'tvanepps', 'i1skn', 'nanopool', \n",
    "            'brave', '045bkp', 'trezor', 'twitter', 'WinterTFG0',\n",
    "            'ethereum', 'TremendouslyHighFrequency', 'menonsamir',\n",
    "            'certusone', 'coreybutler', 'certusone', 'bzminer',\n",
    "            'ScorexFoundation', 'ergoMixer', 'rust-bitcoin', 'non', 'WyvernTKC',\n",
    "            'JohnLaw2', 'nervosnetwork', 's-nomp', 'sp-hash', 'zone117x',\n",
    "            'ShiftLeftSecurity', 'MrMaxweII', 'forknote', 'AtomicLoans',\n",
    "            'Dav-Git', 'starkware-libs', 'reach-sh', 'reflexer-labs',\n",
    "            'aeternity', 'alt3', 'NixOS', 'OpenAPITools', 'Mikerah',\n",
    "            'BySergeyDev', 'sangria-graphql', 'soullesscomputerboy',\n",
    "            'TwiN', 'SundaeSwap-finance', 'minernl', 'WinterTFG',\n",
    "            'JulianKemmerer', 'kyuupichan', 'sdaveas', 'lightbend',\n",
    "            'AlphaX-Projects', 'ma-ha', 'rsmmnt', 'arduino', 'electron',\n",
    "            'C4K3', 'BLAKE2', 'minio', 'ethereum-mining', 'kadena-io',\n",
    "            'wavesplatform', 'plebbit', 'dcSpark', 'john-light', 'JetBrains',\n",
    "            'Eliovp', 'libp2p', 'simerplaha', 'ossu', 'dashevo', 'ethereumclassic',\n",
    "            'ergoplatform', 'btclinux', 'orgs', 'ghostdogpr', 'yuriy0803', 'portable-scala', 'KomodoPlatform',\n",
    "            'tiangolo', 'bwbush', 'CLRX', 'OhGodPet', 'YfryTchsGD', 'Comcast', 'atomiclabs', 'pikvm',\n",
    "            'PyO3', 'Astodialo', 'SpaceXpanse', 'sbt', 'alephium', 'emeraldpay', 'spantaleev', 'japgolly',\n",
    "            'FgForrest', 'jrbender', 'berry-pool', 'Gravity-Tech', 'obolflip', 'SuSy-One', 'OhGodAPet', 'Balbin-Labs',\n",
    "            '.insteadOf', 'hyperledger-labs', 'spaceswapio', 'search', 'prometheus', 'DevSCNinja', 'aragogi', 'bitcoincashorg', 'bcgit', 'gemlink',\n",
    "            'robkorn', 'sininen-taivas', 'ergo', 'zawy12', 'rooooooooob', 'ergomixer', 'akyo8', 'arcnet', 'adventurersdao',\n",
    "            'gsblabsio'\n",
    "            \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to missing_links https://github.com/mhssamadani/ErgoStratumServer.\n",
      "Adding to missing_links https://github.com/ergop\n",
      "Adding to missing_links https://github.com/anon-br/ledger-ergo-js.\n",
      "Adding to missing_links https://github.com/zkastn/ergo-raffle-bot,\n",
      "Adding to missing_links https://github.com/Emurgo/yoroi-frontend\n",
      "Adding to missing_links https://github.com/scalahub/ErgoScriptCompiler\n",
      "Adding to missing_links https://github.com/Emurgo\n",
      "Adding to missing_links https://github.com/scalahub/ErgoScriptCompiler\n",
      "Adding to missing_links https://github.com/nirvanush/whale-alerts-twit...\n",
      "Adding to missing_links https://github.com/mhssamadani/ErgoStratumServer>\n",
      "Adding to missing_links https://github.com/ThierryM1212/SAFEW>\n",
      "Adding to missing_links https://github.com/capt-nemo429/nautilus-wallet>\n",
      "Adding to missing_links https://github.com/abchrisxyz/ergowatch>\n",
      "Adding to missing_links https://github.com/mhssamadani/Autolykos2_NV_Miner.\n",
      "Adding to missing_links https://github.com/andrehafner/my.ergoport.dev>\n",
      "Adding to missing_links https://github.com/Eeysirhc/tidyergo.\n",
      "Adding to missing_links https://github.com/anon-real/ErgoTeam.\n",
      "Adding to missing_links https://github.com/nirvanush/isomorphic-wallet,\n",
      "Adding to missing_links https://github.com/ErgoWallet/ergowallet-desktop,\n",
      "Adding to missing_links https://github.com/networkspore/arcnet\n",
      "Adding to missing_links https://github.com/networkspore/arcnet\n",
      "Adding to missing_links https://github.com/ThierryM1212/transaction-builder.\n",
      "Adding to missing_links https://github.com/Luivatra/ergo.\n",
      "Adding to missing_links https://github.com/scalahub/ErgoScriptCompiler\n",
      "Adding to missing_links https://github.com/ThierryM1212/ergo-token-min...\n",
      "Adding to missing_links https://github.com/mgpai22/ergo-explorer-testnet,\n",
      "Adding to missing_links https://github.com/crystoll/export-erg-transactions...\n",
      "Adding to missing_links https://github.com/gnuion/dedao\n",
      "Adding to missing_links https://github.com/kushti/dexy-stable,\n",
      "Adding to missing_links https://github.com/gnuion/dedao\n",
      "Adding to missing_links https://github.com/abchrisxyz/ergo-setup>\n",
      "Adding to missing_links https://github.com/scalahub/ErgoScriptCompiler\n",
      "Adding to missing_links https://github.com/scalahub/ErgoScriptCompiler\n",
      "Adding to missing_links https://github.com/scalahub/ErgoScriptCompiler\n",
      "Adding to missing_links https://github.com/ross-weir/ergo-ledger-dev,\n",
      "Adding to missing_links https://github.com/anon-real/ErgoUtils.\n",
      "613 existing organizations and repositories found\n",
      "29 missing links found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare the github.com.txt file and create a list of missing links\n",
    "missing_links = set()\n",
    "\n",
    "for link in github_links:\n",
    "    link = remove_special_chars(link.strip())\n",
    "    trimmed_link = trim_url(link)\n",
    "    if is_valid_url(trimmed_link):\n",
    "        match = re.search(r\"github.com/([^/]+)(?:/([^/]+))?\", trimmed_link)\n",
    "\n",
    "        # if the URL is a valid GitHub URL, check if it is already in the list of existing organizations and repositories\n",
    "        if match:\n",
    "            # Extract the organization and repository names from the URL\n",
    "            org_and_repo = match.groups()\n",
    "            # Check if the organization is in the ignore list\n",
    "            if org_and_repo[0] not in ignore_orgs:\n",
    "\n",
    "                # Check if the organization exists in the existing_orgs_and_repos set\n",
    "                org_exists = any(existing_org == org_and_repo[0] and existing_repo is None for existing_org, existing_repo in existing_orgs_and_repos)\n",
    "                \n",
    "                # If the organization doesn't exist, and the URL is not in the existing_urls_set, add it to the missing_links\n",
    "                if not org_exists and trimmed_link not in existing_urls_set:\n",
    "                    print(\"Adding to missing_links\", trimmed_link)\n",
    "                    missing_links.add(trimmed_link)\n",
    "                    existing_orgs_and_repos.add(org_and_repo)\n",
    "                    #print(f\"Added {org_and_repo} to existing_orgs_and_repos\")\n",
    "# Convert the missing_links set back to a list\n",
    "missing_links = list(missing_links)\n",
    "\n",
    "\n",
    "print(len(existing_orgs_and_repos), \"existing organizations and repositories found\")\n",
    "print(len(missing_links), \"missing links found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ergo.toml file\n",
    "with open(output_toml, \"r\") as file:\n",
    "    ergo_data = toml.load(file)\n",
    "\n",
    "# Add the missing links to ergo.toml\n",
    "for link in missing_links:\n",
    "    if is_valid_url(link):\n",
    "        ergo_data[\"repo\"].append({\"url\": link})\n",
    "\n",
    "# Save the changes in the existing ergo.toml file\n",
    "with open(output_toml, \"w\") as file:\n",
    "    toml.dump(ergo_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "import re\n",
    "\n",
    "with open(output_toml, \"r\") as file:\n",
    "    data = toml.load(file)\n",
    "\n",
    "data[\"repo\"] = [d for d in data[\"repo\"] if re.match(r\"^https?://[\\w\\-\\.~:\\#\\?@\\[\\]\\!\\$&'\\(\\)\\*\\+,;=/]+$\", d[\"url\"])]\n",
    "\n",
    "with open(output_toml, \"w\") as file:\n",
    "    toml.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "import glob\n",
    "import toml\n",
    "\n",
    "sub_ecosystem_toml_files = glob.glob(\"../../../crypto-ecosystems/data/ecosystems/*/ecosystem.toml\")\n",
    "\n",
    "existing_orgs_and_repos = set()\n",
    "\n",
    "for toml_file in sub_ecosystem_toml_files:\n",
    "    with open(toml_file, \"r\") as file:\n",
    "        data = toml.load(file)\n",
    "\n",
    "        if \"github_organizations\" in data:\n",
    "            for org in data[\"github_organizations\"]:\n",
    "                existing_orgs_and_repos.add((org.strip(\"/\").split(\"/\")[-1], None))\n",
    "\n",
    "        if \"repo\" in data:\n",
    "            for repo in data[\"repo\"]:\n",
    "                url = repo.get(\"url\")\n",
    "                if url:\n",
    "                    trimmed_url = url.strip(\"/\").replace(\"https://github.com/\", \"\")\n",
    "                    org_and_repo = tuple(trimmed_url.split(\"/\")[:2])\n",
    "                    existing_orgs_and_repos.add(org_and_repo)\n",
    "\n",
    "with open(\"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\", \"r\") as file:\n",
    "    ergo_data = toml.load(file)\n",
    "\n",
    "new_repos = []\n",
    "for repo in ergo_data.get(\"repo\", []):\n",
    "    url = repo.get(\"url\")\n",
    "    if url:\n",
    "        trimmed_url = url.strip(\"/\").replace(\"https://github.com/\", \"\")\n",
    "        org_and_repo = tuple(trimmed_url.split(\"/\")[:2])\n",
    "        if org_and_repo not in existing_orgs_and_repos:\n",
    "            new_repos.append(repo)\n",
    "\n",
    "ergo_data[\"repo\"] = new_repos\n",
    "\n",
    "with open(\"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\", \"w\") as file:\n",
    "    toml.dump(ergo_data, file)\n",
    "\n",
    "def get_toml_path(toml_base_name, base_path):\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == toml_base_name:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import toml\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# Load ergo.toml\n",
    "with open(\"../../../crypto-ecosystems/data/ecosystems/e/ergo.toml\", \"r\") as f:\n",
    "    ergo_data = toml.load(f)\n",
    "\n",
    "# Get sub-ecosystems\n",
    "sub_ecosystems = ergo_data[\"sub_ecosystems\"]\n",
    "\n",
    "# Get repo URLs\n",
    "existing_urls_set = set()\n",
    "for repo in ergo_data.get(\"repo\", []):\n",
    "    existing_urls_set.add(repo.get(\"url\"))\n",
    "\n",
    "# Define the base directory for searching .toml files\n",
    "base_dir = \"../../../crypto-ecosystems/data/ecosystems/\"\n",
    "\n",
    "# Check each sub-ecosystem for duplicate URLs\n",
    "for sub_ecosystem in sub_ecosystems:\n",
    "    toml_base_name = sub_ecosystem.lower().replace(\" \", \"_\") + \".toml\"\n",
    "    toml_path = get_toml_path(toml_base_name, base_dir)\n",
    "    print(sub_ecosystem)\n",
    "    print(toml_path)\n",
    "    if toml_path:\n",
    "        with open(toml_path, \"r\") as f:\n",
    "            data = toml.load(f)\n",
    "        if \"repo\" in data:\n",
    "            for repo in data[\"repo\"]:\n",
    "                if repo.get(\"url\") in existing_urls_set:\n",
    "                    # Remove the duplicate URL from the repo section of ergo.toml\n",
    "                    ergo_data[\"repo\"] = [r for r in ergo_data.get(\"repo\", []) if r.get(\"url\") != repo.get(\"url\")]\n",
    "\n",
    "\n",
    "# Save the changes to ergo.toml\n",
    "with open(\"output.toml\", \"w\") as f:\n",
    "    toml.dump(ergo_data, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
